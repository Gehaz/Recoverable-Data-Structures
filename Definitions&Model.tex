\section{Model and Definitions (based on our PODC)}
\label{section: Model}

%\subsection{Standard Shared-Memory Model}

We consider a system where $N$ asynchronous \textit{processes}
$p_1, \ldots, p_{N}$ communicate by accessing \emph{concurrent objects}.
The system provides \textit{base objects} that support
atomic read, write, and compare\&swap (CAS).
Base objects can be used for implementing more complex concurrent
objects (e.g. lists, trees and stacks), by defining access procedures
that simulate each operation on the implemented object using operations
on base objects.

The state of each process consists of non-volatile \emph{shared-memory variables},
as well as \emph{local variables stored in volatile processor registers}.
Each process can incur at any point during the execution a \emph{crash-failure}
(or simply a \emph{crash}) that resets all its local variables to arbitrary values,
but preserves the values of all its non-volatile variables.
A process $p$ \emph{invokes an operation} $Op$ on an object by performing
an \emph{invocation step}; 
upon \emph{Op}'s completion, a \emph{response step} is executed.

Operation $Op$ is \emph{pending} if it was invoked but was not yet completed.
For simplicity, we assume that, at all times,
each process has at most a single pending operation on any one object.

[[HA: Define data structure $D$.]
Each data structure has an associated \emph{recovery function},
denoted $D.\texttt{Recover}$,
which is responsible for restoring the data structure to a consistent 
state, upon recovery from a crash.

More formally, a \textit{history} $H$ is a sequence of \emph{steps}.
There are four types of steps:
\begin{enumerate}
	\item an \emph{invocation step}, denoted $(INV, p, O, Op)$, represents the invocation by process $p$ of operation $Op$ on object $O$;
	\item an operation $Op$ can be completed either normally or when, following one or more crashes, the execution of $Op.\texttt{Recover}$ is completed. In either case, a \emph{response step} $s$, denoted $(RES, p, O, Op, ret)$, represents the completion by process $p$ of operation $Op$ invoked on object $O$ by some step $s'$ of $p$, with response $ret$ being written to a local variable of $p$. We say that \emph{s is the response step that matches s'};
	\item a \emph{crash step} $s$, denoted $(CRASH, p)$, represents the crash of process $p$. We call the inner-most recoverable operation $Op$ of $p$ that was pending when the crash occurred the \emph{crashed operation of s}. $(CRASH, p)$ may also occur while $p$ is executing some recovery function $Op.\texttt{Recover}$ and we say that $Op$ is the crashed operation of $s$ also in this case;
	\item a \emph{recovery step $s$ for process p}, denoted $(REC, p)$, is the only step by $p$ that is allowed to follow a $(CRASH, p)$ step $s'$. It represents the resurrection of $p$ by the system, in which it invokes $Op.\texttt{Recover}$,\footnote{A history does not contain invocation/response steps for recovery functions.} where $Op$ is the crashed operation of $s'$.
We say that \emph{s is the recovery step that matches} $s'$.
\end{enumerate}

For a history $H$, we let $H | p$ denote the subhistory of $H$ consisting of all the steps by process $p$ in $H$. 
$H$ is \emph{crash-free} if it contains no crash steps (hence also no recover steps). 
We let $H|{<}p,O{>}$ denote the subhistory consisting of all the steps on $O$ by $p$.

Given two operations $op_1$ and $op_2$ in a history $H$,
we say that $op_1$ \textit{happens before} $op_2$,
denoted $op_1 <_H op_2$,
if $op_1$'s response step precedes the invocation step of $op_2$ in $H$.
$H | O$ is a \emph{sequential object history},
if it is an alternating series of invocations and the matching responses
starting with an invocation (the history may end by a pending invocation).
The \textit{sequential specification} of an object $O$ is the set of all
possible (legal) sequential histories over $O$.
A history $H$ is \emph{sequential} if $H | O$ is a sequential object
history for all objects $O$.

Two histories $H$ and $H'$ are \emph{equivalent},
if $H|{<}p,O{>}=H'|{<}p,O{>}$ for all processes $p$ and objects $O$.
Given a history $H$, a \textit{completion} of $H$ is a history $H'$
constructed from $H$ by selecting separately,
for each object $O$ that appears in $H$,
a subset of the operations pending on $O$ in $H$ and appending matching
responses to all these operations, and then
removing all remaining pending operations on $O$ (if any).

\begin{definition} [Linearizability \cite{herlihyWingLinearizability}, rephrased]
	\label{Definition: Linearizability}
A finite crash-free history $H$ is \emph{linearizable} if it has a completion $H'$ and a legal sequential history $S$ such that:
	\begin{enumerate}
		\item [L1.] $H'$ is equivalent to $S$; and
		\item [L2.] $<_H \subseteq <_S$ (i.e., if $op_1 <_H op_2$ and both ops appear in $S$ then $op_1 <_S op_2$).
	\end{enumerate}
\end{definition}

Thus, a finite history is linearizable, if we can linearize the subhistory of each object that appears in it.
Next, we define a more general notion of well-formedness that applies also to histories that contain crash/recovery steps. For a history $H$, we let $N(H)$ denote the history obtained from $H$ by removing all crash and recovery steps.

\begin{definition} [Recoverable Well-Formedness]
\label{def:recoverable-well-formedness}
A history $H$ is \textit{recoverable well-formed} if the following holds.
\begin{enumerate}
\item Every crash step in $H | p$ is either $p$'s last step in $H$ or is followed in $H | p$ by a matching recover step of $p$.
\item $N(H)$ is well-formed.
\end{enumerate}
\end{definition}

We can now define the notion of nesting-safe recoverable linearizability.

\begin{definition} [Nesting-safe Recoverable Linearizability (NRL)]
\label{Definition:NRL}
A finite history $H$ satisfies \emph{nesting-safe recoverable linearizability} (NRL) if it is recoverable well-formed and $N(H)$ is a linearizable history.
An object implementation satisfies NRL if all of its finite histories satisfy NRL.
\end{definition}



\subsection{General Overview}
Designing a data structure which is persistent in the presence of a crash-recovery is not an easy task. Several different correctness conditions and implementations have been proposed for such data structures [cite...]. However, most of these conditions and implementations does not support detectability, that is, the ability to conclude upon recovery whether the crashed operation took effect or not.

Attiya et al [cite] presented a novel crash-recovery model together with a correctness condition. Roughly speaking, Nesting-safe Recoverable Linearizability (NRL) requires each recoverable operation to supply an recover function, such that invoking it after a crash inside the operation allows the process to complete the operation, as well as restore the response value if needed. Moreover, they presented recoverable implementations for read,write and CAS operations. As suggested by the name NRL, it allows nesting. Therefore, taking any algorithm which uses only read,write and CAS primitives, and replacing each primitive with its recoverable version yields an NRL implementation (some minor changes are still needed in order to use this transformation). In particular, this is the case for all the algorithms presented in this paper. However, this transformation is very costly, in terms of both time and space.

Cohen et al [cite..] presented a universal construction that implements durably any object in a lock-free manner, using at most one persistent fence per operation, which is optimal. Moreover, the algorithm uses only read, write and CAS operations. However, universal construction by its nature is not efficient. In particular, their implementation requires to keep the entire history of the object in a designated shared queue, as well as per-process persistent log, such that these logs as whole keeps the entire history, and different logs may have a big overlap. In addition, the response of an operation is determine by looking at the entire history up to the linearization point of the operation. Their universal construction can be made detectable in the following way: upon recovery from a crash inside an $Update(op)$ operation, and after the system completes its recovery routine, $op$ was linearized if and only if $op$ appears in the shared queue (assuming all $op$s are unique). In such case, the process can conclude the response value, since the queue represents the object's history.

Clearly, although the construction has an optimal fence complexity, in practice it is not efficient. Moreover, the crash model they consider is a system-wide crash, in which all processes crash at once. In such case, upon recovery, a single recover function is being executed by the system in order to reconstruct the queue data-structure in a consistent way. The correctness condition they consider is durable linearizability [cite], which requires that after a full system crash, the state of the object must reï¬‚ect a consistent operation sub-history that includes all operations completed by the time of the crash. Therefore, some of the crashed operation may get lost.

Different concurrent implementation for specific data-structures are known. These implementations exploit the structure and requirements of the specific object in order to optimize the resulting implementation. For example, operations that effects different parts of the data structure can be done concurrently without interfering each other. A lot of work and effort has been put in order to develop these implementations, as well as proving their correctness, thus we would like to have a general approach to turn a given implementation to a recoverable one, while preserving its structure and complexity as much as possible. This way, we can enjoy both an efficient algorithm, as well as recoverability, while avoiding the need to design new algorithms.

Ben-David, Blelloch and Wei [ref] presented a transformation for implementations which uses only read, write and CAS operation. Their transformation results a recoverable and detectable implementation. The approach for the transformation is similar to the NRL transformation using recoverable base objects [our paper ref?] - it split the code into capsules, each contains a single CAS operation followed by reads, and then replacing each CAS with its recoverable version. Each capsule can be recovered in case of a crash inside the capsule. In addition, the paper presents an optimisation for normalized algorithms, such that only two capsules are required for any operation. However, not all implementation we consider are given in a normalized form, and the process of converting an implementation into a normalized form may be costly by itself.

Since Ben-David at el transformation is general, it may be improved in certain cases. In more details, each CAS is replaced with a recoverable CAS. This require each CAS operation to use different arguments (they implemented it using an increasing sequence number), which will be stored in the CAS location. This in turn implies that the CAS uses an unbounded word length, even if the original implementation uses a bounded CAS (for example, the operations are over a finite domain). Furthermore, altohugh two capsules are used for each normalized operation, these two capsules are executed repeatedly, in each attempt of performing the operation (the implementation is lock-free, hence a process can fail to complete its operation over and over). In many cases, we can avoid it, and not paying the extra complexity of the capsules for a failed attempt.
In addition, we would like the transformation to change as little as necessary from the original code, even in the price of having a costly recover function. Assuming crash is a rare event, this may result a more efficient implementation in practice.








In this paper we present a new recoverable implementation for Harris' Linked-List. This implementation is efficient in terms of both time and space, and avoids few of the disadvantages the above general constructions have. In particular, our implementation does not use a recoverable CAS, but rather a primitive one. Also, the content of a CAS object is the same as the original algorithm, while in Ben-David et al implementation a recoverable CAS stores also an unbounded sequence number. Under the assumption that crash is a rare event, we have tried to make the regular operations as efficient as possible, while willing to pay with a more complex recover functions. In more details, the recovery function may access the data structure and perform operations that does not change its structure, such as search.

Designing a recoverable implementation for a specific data structure, even though may result an efficient algorithm, is a non trivial and time consuming task. However, our implementation exploit a certain structure that can be found on other data structures as well, and thus our technique can be used in those cases as well. More specific, many data structures uses a CAS primitive in order to sync the operations, such that each operation is linearized at the time of a single successful CAS. In the Linked-List, for example, Insert operation is linearized at the point of adding a pointer to the new node using a successful CAS, while Delete operation is linearized at the point of a successful logical delete. The key point in recovering is that if a process crash when about to perform a CAS operation, it is not trivial to discover upon recovery whether its operation took effect. Moreover, two or more process can try and perform the same operation at the same time, for example, deleting the same node. Upon recovery, a process can not tell whether it is the process to successfully perform the operation, or that it was done by some other process, hence it does not know what value to return. Therefore, a mechanism to determine the ID of the process which performed the operation is needed, such that out of all processes attempting to perform it, exactly one is defined to successfully performed it.
