


\section{Elimination Stack}
\label{section:elimination-stack}

For simplicity, we assume a value \init, which is different from \NULL\ and any other value the stack can store. Since \NULL\ is used as a legit return value, representing the value of \pop\ operation (when exchanging values using the elimination array), \NULL\ can not be used to represent an initialization value, different then any stack value. The same holds for a Node, since a \NULL\ node represent an empty stack, the value \init\ is used to distinguish between initialization value and empty stack.

For simplicity, we split the \recover\ routine into sub-routines, based on which operation (\push, \pop, \exchange) is pending, or needs to be recover. This can be concluded easily by the type of record stored in $Announce[pid]$ (\exInfo\ or \opInfo), thus there is no need to explicitly know where exactly in the code the crash took place. Also, the \recover\ routine returns \fail\ in case the last pending operation did not took affect (no linearization point), nor it will take in any future run. In such case, the user has the option to either re-invoke the operation, or to skip it, depends on the needs and circumstances of the specific use of the data structure.

The given implementation ignores the log of failures and successes of the exchange routine when recovering. That is, in case of a crash during an \exchange, a process is able to recover the \exchange\ routine, however, the log of successes and failures is not update, since it might be the process already updated it. In addition, in case of a \fail\ response, we do not know whether the time limit (timeout) was reached, or that the process simply crashed earlier in the routine without completing it. The given implementation can be expanded to also consider the log. Nonetheless, for ease of presentation we do not handle the log in case of a crash. Assuming crash events are rare, the log still gives a roughly good approximation to the number of failures and successes, thus our approach might be useful in practice.

\subsection{A Lock-Free Exchanger}
An exchanger object supports the \exchange\ procedure, which allows exactly two processes to exchange values.
If process A calls the \exchange\ with argument $a$, and B calls the \exchange\ of the same object with argument $b$, then A's call will return value $b$ and vice versa.

On the original algorithm [cite the book?!], processes race to win the exchanger using a \CAS\ primitive. A process accessing the exchanger first reads its content, and act according to the state of it. The first process observe an \emptyst\ state, and tries to atomically writes its value and change the state to \waiting. In such case, it spins and wait for the second process to arrive. The second, observing the state is now \waiting, tries to write its value and change the state to \busy. This way, it informs the first one a successful collision took place. Once the first process notice the collision, it reads the other process value and release the exchanger by setting it back to \emptyst.
In order to avoid an unbounded waiting, if a second process does not show up, the call eventually timeout, and the process release the exchanger and return.

Assume a process $p$ successfuly capture the exchanger by setting its status to \waiting, followed by a crash. Now, some other process $q$ complete the exchange by setting the exchanger to \busy. Upon recovery, $p$ can conclude some exchange was completed, but it can not tell whether its value is part of the exchange, and thus it can not complete the operation. Moreover, $p$ and $q$ must agree, otherwise $q$ will return $p$'s value, and thus the operation of $p$ must be linearized together with $q$ operation.

In order to avoid the above problem, we take an approach resembling the BST implementation. Instead of writing a value to the exchanger, processes will use an info record, containing the relevant information for the exchange. This way, processes use the exchanger in order to exchange info records (more precisely, pointers to such records), and not values. To overcome the problematic scenario described earlier, if a process $q$ observe the exchanger state is \waiting\ with some record $yourop$, it first update its own record $myop$ it is about to try and collide with $yourop$, and only then performs the \CAS. This way, if the collision is successful, the record $myop$ which now stored in the exchanger implies which two records collide. Also, the fact that different processes uses different records guarantee that at most one record can collide with $yourop$.

Using records instead of values, when using wisely, allows us to farther improve the algorithm. First, there is no need to store the exchanger's state in it (by using 2 bits of it to mark the state), but we can rather have this info in the record. Second, if there is a \busy\ record in the exchanger, it contains the info of the two colliding records. Therefore, a third process, trying to also use the exchanger, can help the processes to complete the collision, and then can try and set the exchanger back to \emptyst, so it can use it again. In the original implementation, a process observaing a \busy\ exchanger, have to wait for the first process to read the value and release the exchanger. Therefore, if the first process crash after the collision, the exchanger will be hold by it forever. The helping mechanism avoids this scenario, making the exchange routine non-blocking.

Notice that no exchange record with \emptyst\ state is ever created, except for the $default$ record. Therefore, reading \emptyst\ state is equivalent to the exchanger storing a pointer to $default$. A process $p$ creates a new record $myop$ when accessing the exchanger, with a unique address. As long as $p$ fails to perform a successful \CAS, and thus fails to store $myop$ in $slot$, it is allowed to try again. However, once a process performs a successful \CAS\ and stores $myop$ in $slot$, the only other \CAS\ it is allowed to do are in order to try and store $defualt$ in $slot$. Thus, $myop$ can be written exactly once to $slot$. It follows that a collision can occur between two processes exactly - once a \waiting\ record stored in $slot$, only a single \CAS\ can replace it with a \busy\ record. As the two records can not be written again to $slot$, no other process can collide with any of the records.

The \recoverExchange\ routine relies on the following argument. If a process $p$ successfully wrote $op_p$ to $slot$ using the \CAS\ in line~\ref{exchage-waiting-cas}, the only way to overwrite it by a different process $q$, is by a \CAS\ in line~\ref{exchange-busy-cas} with a record $op_q$ such that its state is \busy, and $op_q.partner = op_p$. In addition, the only way to overwrite $op_q$ is by a \CAS\ replacing it with $default$, and this is done only after \switchPair$(op_p, op_q)$ is completed, and thus both $result$ fields are updated.

The correctness of the \recoverExchange\ routine is based on the above argument. There are few scenarios to consider.
If $p$ crash after a successful \CAS\ in line~\ref{exchage-waiting-cas}, then $op_p$ state is \waiting. Therefore, when reading $slot$ in the \recoverExchange\ one of the following must hold. If $slot$ contains $op_p$, then no process collide with $p$, and $p$ continue to run as if the time limit has been reached. Otherwise, there was a collision. From the above argument, it must be that either $op_q$ that collide with $op_p$ is stored in $slot$, in this case $op_q.partner = op_p$, and $p$ will try to complete the collision and release $slot$, or that $op_q$ has been overwritten, and in this case the $result$ field of $op_p$ is updated. In both cases, $p$ returns $op_p.result$.
If $p$ crash after a successful \CAS\ in line~\ref{exchange-busy-cas}, then $op_p$ state is \busy. It follows from the argument that the only way to overwrite $op_p$ is only after completing the collision by \switchPair. Thus, either upon recovery $p$ reads $op_p$ from $slot$, and in this case it tries to complete the the operation, or that $op_p.result$ was already updated. In both cases, $p$ returns it.
If non of the above holds, then $op_p$ was not involved in any collision, because either no successful \CAS\ was done by $p$, or $p$ reached the time limit while no process show up, and was able to set $slot$ back to $defualt$. In any case, after the crash of $p$, $op_p$ will never be written again to $slot$, nor any other $op_q$ such that $op_q.partner = op_p$, as any such $op_q$ tries to perform \CAS$(op_p,op_q)$ that will fail. Also, as no process can collide with $op_p$, no \switchPair\ with $op_p$ as parameter is ever invoked, and in particular $op_p.result = \init$ for the rest of the execution. This in turn implies that upon recovery $p$ will return \fail, as required.


\subsection{Lock-Free Stack}

The stack implementation is due to [....]. The \trypush\ routine tries to atomically have a new node pointing to the old top, and then updating the top to be the new node. The \trypop\ routine tries to atomically read the top of the stack, and change the top to the next node of it. The two routines uses \CAS\ in order to gurantee no change for the top was made between the read and write.
\push\ (resp. \pop) routine is alternating between a \trypush\ (\trypop) routine, which access the central stack, and the \exchange, trying to collide with an opposite operation.

In order to make the implementation recoverable, we need a way to infer whether a \pop\ or \push\ already took affect, in case of a crash. Moreover, in case of a \pop, we also need to infer which process is the one to pop the node. For that, we use an approach similar to the Linked-List implementation. Each node contains a new field $popby$ which is used to identify a \push\ of the node completed, as well as a \pop\ of the node was completed, and who is the process to pop it.
Consider the following scenario. Assume a process $p$ performs a \push\ operation with node $nd$, and using a \CAS\ succeed to update the stack top to point to $nd$, followed by a crash. Now, process $q$ performing a \pop\ operation performs a \CAS\ causing the removal of $nd$ from the stack (by changing top to the next node). In this case, once $p$ recovers, $nd$ is no longer part of the stack, and it is also not marked as deleted. This is indistinguishable from a configuration in which the \push\ of $nd$ was yet to take affect (a crash before \CAS), and thus $p$ can not know what the right response is.

One way to solve this issue is by first marking a node for removal, and only then remove it. This way, if a node is no longer part of the stack it must be marked, and thus we can conclude it was in the stack, and the \push\ routine was successful. However, such an implementation, in addition for the need of to system to support a markable reference, also requires process to help each other. If a node is marked for delete, then a process trying to perform a different operation first needs to complete the deletion, before applying its own operation, otherwise the physical delete of the node may not take place, leaving the node forever in the stack. As the original algorithm avoids any marking, and simply tries to swing the $Top$ pointer, we would like to maintain this property.

A field $popby$ is initialised to $\init$ when a node $nd$ is created. Once the node is successfully insert to the stack by a \push\ operation, the inserting process tries to mark it by changing $popby$ to \NULL\ using a \CAS. Before a process tries to remove the node from the stack during a \pop\ routine, it first mark it as part of the stack by doing the same thing, helping the inserting process conclude the node is in the stack. This replace the logic delete of the node, as we only need to know the node was part of the stack if it is removed. After a successful \CAS\ to remove $nd$ from the stack, another \CAS\ is used in order to try and set $popby$ to the identifier of the process who performed the \CAS. The use of \CAS\ to change $popby$ from \init to \NULL, and from \NULL\ to an identifier guarantee that only the first process to perform each of these \CAS\ will succeed. Note that before writing an identifier to $popby$ a process must try and set it to \NULL, and thus it can not store two different identifiers along in any execution.

The correctness proof follows the same guidelines as of the proof for the Linked-List. If a \push\ operation did not introduce a new node $nd$ into the stack, then no process but $p$ is aware of $nd$. Thus, upon recovery the \search\ routine will not find $nd$ in the stack, nor its $popby$ field has been changed, and the \recoverPush\ returns \fail. Otherwise, $nd$ was successfully inserted to the stack. As discessed above, the only way to delete $nd$ from the stack is by first changing its $popby$ field to \NULL. Thus, upon recovery $p$ will either find $nd$ in the stack, using the \search\ routine, or that $popby$ is different then $\init$ in case it was deleted, and in both cases it returns \True.
For the \pop\ routine, if $p$ tries to remove a node $nd$ from the top of the stack and crash, then upon recovery it first check if $nd$ is still in the stack using the \search\ routine. If it is so, then clearly $nd$ was yet to delete, and it returns \fail. Otherwise, $nd$ was deleted, either by $p$ or by some other process. Only the first process of which to performs a \CAS, writing its identifier to $popby$ will return the value stored in $nd$, while the others return either \init\ (in the \trypop\ routine) or \fail\ (in the \recoverPop\ routine).

Notice that both \recoverPush\ and \recoverPop\ are wait-free. Due to the structure of stack, no $next$ pointer of any node in the stack is ever changed. Therefore, once a process reads $Top$ at the beginning of its \recover\ routine, the chain of pointers from this $Top$ to the last node in the stack is fixed for the rest of the execution, and thus traversing it using the \search\ routine is wait-free.



