\section{Model and Definitions (based on our PODC)}
\label{section: Model}

%\subsection{Standard Shared-Memory Model}

We consider a system where $N$ asynchronous \textit{processes}
$p_1, \ldots, p_{N}$ communicate by accessing \emph{concurrent objects}.
The system provides \textit{base objects} that support
atomic read, write, and compare\&swap (CAS).
Base objects can be used for implementing more complex concurrent
objects (e.g. lists, trees and stacks), by defining access procedures
that simulate each operation on the implemented object using operations
on base objects.

The state of each process consists of non-volatile \emph{shared-memory variables},
as well as \emph{local variables stored in volatile processor registers}.
Each process can incur at any point during the execution a \emph{crash-failure}
(or simply a \emph{crash}) that resets all its local variables to arbitrary values,
but preserves the values of all its non-volatile variables.
A process $p$ \emph{invokes an operation} $Op$ on an object by performing
an \emph{invocation step}; 
upon \emph{Op}'s completion, a \emph{response step} is executed.

Operation $Op$ is \emph{pending} if it was invoked but was not yet completed.
For simplicity, we assume that, at all times,
each process has at most a single pending operation on any one object.

[[HA: Define data structure $D$.]
Each data structure has an associated \emph{recovery function},
denoted $D.\texttt{Recover}$,
which is responsible for restoring the data structure to a consistent 
state, upon recovery from a crash.

More formally, a \textit{history} $H$ is a sequence of \emph{steps}.
There are four types of steps:
\begin{enumerate}
	\item an \emph{invocation step}, denoted $(INV, p, O, Op)$, represents the invocation by process $p$ of operation $Op$ on object $O$;
	\item an operation $Op$ can be completed either normally or when, following one or more crashes, the execution of $Op.\texttt{Recover}$ is completed. In either case, a \emph{response step} $s$, denoted $(RES, p, O, Op, ret)$, represents the completion by process $p$ of operation $Op$ invoked on object $O$ by some step $s'$ of $p$, with response $ret$ being written to a local variable of $p$. We say that \emph{s is the response step that matches s'};
	\item a \emph{crash step} $s$, denoted $(CRASH, p)$, represents the crash of process $p$. We call the inner-most recoverable operation $Op$ of $p$ that was pending when the crash occurred the \emph{crashed operation of s}. $(CRASH, p)$ may also occur while $p$ is executing some recovery function $Op.\texttt{Recover}$ and we say that $Op$ is the crashed operation of $s$ also in this case;
	\item a \emph{recovery step $s$ for process p}, denoted $(REC, p)$, is the only step by $p$ that is allowed to follow a $(CRASH, p)$ step $s'$. It represents the resurrection of $p$ by the system, in which it invokes $Op.\texttt{Recover}$,\footnote{A history does not contain invocation/response steps for recovery functions.} where $Op$ is the crashed operation of $s'$.
We say that \emph{s is the recovery step that matches} $s'$.
\end{enumerate}

For a history $H$, we let $H | p$ denote the subhistory of $H$ consisting of all the steps by process $p$ in $H$. 
$H$ is \emph{crash-free} if it contains no crash steps (hence also no recover steps). 
We let $H|{<}p,O{>}$ denote the subhistory consisting of all the steps on $O$ by $p$.

Given two operations $op_1$ and $op_2$ in a history $H$,
we say that $op_1$ \textit{happens before} $op_2$,
denoted $op_1 <_H op_2$,
if $op_1$'s response step precedes the invocation step of $op_2$ in $H$.
$H | O$ is a \emph{sequential object history},
if it is an alternating series of invocations and the matching responses
starting with an invocation (the history may end by a pending invocation).
The \textit{sequential specification} of an object $O$ is the set of all
possible (legal) sequential histories over $O$.
A history $H$ is \emph{sequential} if $H | O$ is a sequential object
history for all objects $O$.

Two histories $H$ and $H'$ are \emph{equivalent},
if $H|{<}p,O{>}=H'|{<}p,O{>}$ for all processes $p$ and objects $O$.
Given a history $H$, a \textit{completion} of $H$ is a history $H'$
constructed from $H$ by selecting separately,
for each object $O$ that appears in $H$,
a subset of the operations pending on $O$ in $H$ and appending matching
responses to all these operations, and then
removing all remaining pending operations on $O$ (if any).

\begin{definition} [Linearizability \cite{herlihyWingLinearizability}, rephrased]
	\label{Definition: Linearizability}
A finite crash-free history $H$ is \emph{linearizable} if it has a completion $H'$ and a legal sequential history $S$ such that:
	\begin{enumerate}
		\item [L1.] $H'$ is equivalent to $S$; and
		\item [L2.] $<_H \subseteq <_S$ (i.e., if $op_1 <_H op_2$ and both ops appear in $S$ then $op_1 <_S op_2$).
	\end{enumerate}
\end{definition}

Thus, a finite history is linearizable, if we can linearize the subhistory of each object that appears in it.
Next, we define a more general notion of well-formedness that applies also to histories that contain crash/recovery steps. For a history $H$, we let $N(H)$ denote the history obtained from $H$ by removing all crash and recovery steps.

\begin{definition} [Recoverable Well-Formedness]
\label{def:recoverable-well-formedness}
A history $H$ is \textit{recoverable well-formed} if the following holds.
\begin{enumerate}
\item Every crash step in $H | p$ is either $p$'s last step in $H$ or is followed in $H | p$ by a matching recover step of $p$.
\item $N(H)$ is well-formed.
\end{enumerate}
\end{definition}

We can now define the notion of nesting-safe recoverable linearizability.

\begin{definition} [Nesting-safe Recoverable Linearizability (NRL)]
\label{Definition:NRL}
A finite history $H$ satisfies \emph{nesting-safe recoverable linearizability} (NRL) if it is recoverable well-formed and $N(H)$ is a linearizable history.
An object implementation satisfies NRL if all of its finite histories satisfy NRL.
\end{definition}



\subsection{General Overview}
Designing a data structure which is persistent in the presence of a crash-recovery is not an easy task. Several different correctness conditions and implementations have been proposed for such data structures [cite...]. However, most of these conditions and implementations does not support detectability, that is, the ability to conclude upon recovery whether the crashed operation took effect or not.

Attiya et al [cite] presented a novel crash-recovery model together with a correctness condition. Roughly speaking, Nesting-safe Recoverable Linearizability (NRL) requires each recoverable operation to supply an recover function, such that invoking it after a crash inside the operation allows the process to complete the operation, as well as restore the response value if needed. Moreover, they presented recoverable implementations for read,write and CAS operations. As suggested by the name NRL, it allows nesting. Therefore, taking any algorithm which uses only read,write and CAS primitives, and replacing each primitive with its recoverable version yields an NRL implementation (some minor changes are still needed in order to use this transformation). In particular, this is the case for all the algorithms presented in this paper. However, this transformation is very costly, in terms of both time and space.

Cohen et al [cite..] presented a universal construction that implements durably any object in a lock-free manner, using at most one persistent fence per operation, which is optimal. Moreover, the algorithm uses only read, write and CAS operations. However, universal construction by its nature is not efficient. In particular, their implementation requires to keep the entire history of the object in a designated shared queue, as well as per-process persistent log, such that these logs as whole keeps the entire history, and different logs may have a big overlap. In addition, the response of an operation is determine by looking at the entire history up to the linearization point of the operation. Their universal construction can be made detectable in the following way: upon recovery from a crash inside an $Update(op)$ operation, and after the system completes its recovery routine, $op$ was linearized if and only if $op$ appears in the shared queue (assuming each $op$ structure has a unique address). In such case, the process can conclude the response value, since the queue represents the object's history.

Clearly, although the construction has an optimal fence complexity, in practice it is not efficient. Moreover, the crash model they consider is a system-wide crash, in which all processes crash at once. In such case, upon recovery, a single recover function is being executed by the system in order to reconstruct the queue data-structure in a consistent way. The correctness condition they consider is durable linearizability [cite], which requires that after a full system crash, the state of the object must reï¬‚ect a consistent operation sub-history that includes all operations completed by the time of the crash. Therefore, some of the crashed operation may get lost.

Different concurrent implementation for specific data-structures are known. These implementations exploit the structure and requirements of the specific object in order to optimize the resulting implementation. For example, operations that effects different parts of the data structure can be done concurrently without interfering each other. A lot of work and effort has been put in order to develop these implementations, as well as proving their correctness, thus we would like to have a general approach to turn a given implementation to a recoverable one, while preserving its structure and complexity as much as possible. This way, we can enjoy both an efficient algorithm, as well as recoverability, while avoiding the need to design new algorithms.

Ben-David, Blelloch and Wei [ref] presented a transformation for implementations which uses only read, write and CAS operation, which results a recoverable and detectable implementation. The transformation splits the code into capsules, each contains a single CAS operation followed by reads, and then replacing each CAS with its recoverable version. Each capsule can be recovered in case of a crash inside the capsule. In addition, the paper presents an optimisation for normalized algorithms, such that only two capsules are required for any operation. However, not all implementation are given in a normalized form, and the process of converting an implementation into a normalized form may be costly by itself.

Since Ben-David at el transformation is general, it may be improved in certain cases. In more details, each CAS is replaced with a recoverable CAS. This requires each CAS operation to use different arguments (they implement it using an increasing sequence number), which will be stored in the CAS location. This in turn implies that the CAS uses an unbounded word length, even if the original implementation uses a bounded CAS (for example, the operations are over a finite domain). Furthermore, altohugh two capsules are used for each normalized operation, these two capsules are executed repeatedly, in each attempt of performing the operation (the implementation is lock-free, hence a process can fail to complete its operation over and over). In many cases we can avoid it, and not paying the extra complexity of the capsules for a failed attempt.
In addition, we would like the transformation to change as little as necessary from the original code, even in the price of having a costly recover function. Assuming crash is a rare event, this may result a more efficient implementation in practice.


\subsubsection*{Linked-List}
Harris' Linked-List presented in details in Section [cite]. The algorithm has the following property, which makes it persistent to crashes - each operation is linearized at its first successful CAS (not including CAS which help other operation). However, as discussed before, this does not imply it is also detectable. The main issue is a crash right after such a CAS. Upon recovery the crashed process needs to know whether the last CAS was successful or not, in order to conclude the right response value. Moreover, two or more processes can try and perform the same operation at the same time, for example, deleting the same node. Therefore, upon recovery $p$ not only needs to know whether the node was deleted, but also to determine if it was performed by it, or by some other process. In order to solve these issues we use the structure of the Linked-List implementation.

First, we notice that, as in many algorithms, data is added to the Linked-List using nodes, and each node is unique (different nodes may have the same key, but must have different references). Therefore, if a new node $nd$ is added to the list using a CAS, then from this point on $nd$ is accessible as part of the list, until it is deleted. This implies a recovery function for the Insert operation, by testing the former two conditions $p$ can conclude whether its last attempt to add $nd$ was successful. For the Delete operation, we notice that once a node is logically delete (the marked bit is turned on), it remains so forever. Hence, if process $p$ tries to delete node $nd$ and crash, upon recovery $p$ can know if the node was logically deleted by looking at the marked bit. However, this is not enough information, as there might be many processes trying to delete $nd$ at the same time, and exactly one needs to succeed. For that, each node is equipped with an extra field named deleter which is initialised to $\bot$. This field is used by processes trying to delete the same node $nd$ in order to reach a consensus regarding which process is the one to delete $nd$. After $nd$ is logically delete, the first process which writes to deleter its id (using an atomic CAS) is the one to declare its Delete operation as successful, while all others return false. Notice that crashed operation are also trying to win the consensus, and that deleter is written to exactly once. If a crash occurs after deleter was written to, then a process can recover by reading deleter and compare it to its own id.

Although the given implementation is specific for Linked-List, the same technique can be used for other implementations which share some similarities with the Linked-List implementation. For example, an operation which takes effect using a single CAS, and needs to guarantee exactly one process is the one to perform it, can use a new field (such as the deleter) in order to determine which process executed it. This requires the ability to conclude whether some process successfully performed the CAS upon recovery. In addition, the Insert operation of data-structures in which a new and unique object is used each time a new key is added (such as a node), can be recovered by looking for the new object, or an indication for it being removed from the data-structure. For example, Michel-Scott queue is such an implementation.


\subsubsection*{Binary Search Tree}

Ellen et al [cite] presented a lock-free implementation for BST using a single-word CAS (see Section [ref...] for the full algorithm). The main idea is that each operation saves its relevant data in a unique info record. A process first declare its operation by flagging and writing its record (using a single atomic CAS) to all relevant nodes, and then the operation is carried out. This way, processes can help each other - a flagged node contains an info record for the operation, such that any process can complete it.

Algorithms in which helping mechanism is used, in many cases implies that many processes can try and perform the same operation concurrently, where only the invoking process will consider it as completed. Moreover, multiple attempts of the same process $p$ may be indistinguishable from a run in which many processes tries to help $p$ complete its operation, as the helping is anonymous. This already implies that in case of a crash a process may recover by retry its own operation according to it last info record. Once a process marks a node, it remains so until it is unmarked because the operation either complete or failed (by the process or any helping process). Therefore, upon recovery $p$ checks to see if the node is still marked with its own operation. If so, it will try to complete the operation, starting from right after the marking CAS. Otherwise, it will restart the operation.

The recover scheme describe above may still result a scenario in which $p$ crash right after completing its operation and unmarking the node. Thus, upon recovery $p$ will restart the operation, causing it to take effect twice. In order to avoid this scenario we update a field in the info record which signifies the operation is done. A key point is to update this field before the cleaning phase in which the unmarking is done. This way, if upon recovery the node is not marked with the operation, then the new field allows $p$ to conclude the reason, that is, if the operation has been completed or failed. Notice that if process crash after updating the done field and before cleaning, upon recovery it still observes a marked node (unless some other process performed the cleaning), thus it will try to complete the operation. Therefore, a process may return only after the cleaning phase is done.

One advantage our approach has is that it relies on several key properties the implementation have, and not necessarily on the type of base objects. Mainly, the fact that each attempt to perform an operation uses a unique info record, so to avoid the ABA problem. Therefore, the CAS operation can be replaced with the LL/SC instruction with some minor modifications. In such case, the recoverable version is still correct, as the new implementation delivers the same guarantees as the original.

The simple but efficient approach we demonstrate for BST can be used for other implementation in which a process first declare its operation by marking a node and adding an info record. Moreover, in some cases it is very natural to transform the implementation to such a form, that is, to perform operations by writing info records that contains the entire data required for the operation (as demonstrate in Section [ref] using the elimination stack). This way, we can both be able to recover, as well as be able to design an helping mechanism. Note that we require each attempt to create a new info record, to avoid the ABA problem.


\subsubsection*{Elimination Stack}
Finally, we present a recoverable implementation for elimination stack (full description can be found in Section [ref]). This is used in order to demonstrate the two techniques presented for Linked-List and BST can be used in other data-structures as well. The elimination stack implementation is composed of two components - the central stack, and an elimination array.

The central stack is implemented in a way which resembles Harris' Linked-List. In order to push or pop the process tries to atomically swing the head pointer using a CAS. Therefore, the same approach taken in the Linked-List can be used for this case also. In more details, the Pop operation will use a new $popby$ field in a way similar to the deleter field, in order to determine which process is the one to pop the node.

However, unlike the Linked-List where a node is first marked before removing it from the list, in the stack a Pop operation is done using a single CAS without the need to mark. Hence, if a process $p$ crash right after successfully adding a new node (using a Push operation), in case the node has been removed from the list, $p$ can not distinguish the resulting configuration from a one in which the Push operation has failed. We address this issue by adding a simple helping mechanism, where a process marks a node as part of the list before trying to pop it. Notice that this mark does not imply the node has been removed, as the Pop operation may fail. For efficiency, we use the $popby$ field for this marking. A process first writes NULL to it, and in case of a successful Pop try to write its id. This is all done using CAS, so to avoid overwriting.

In the elimination array each entry is an exchanger object which allows exactly two processes to exchange values. First, we transform the implementation such that processes are now try to trade info records instead of values. Each process creates an info record containing its value. The exchange is done using a CAS, the first process to perform CAS writes its info record to the exchanger, and the second replace it with its own record. In order to allow recovery, the info record contains a state, identifying whether $p$ is the first or second process to write the exchanger. Also, in case it is the second, the record also contains a reference to the info record $p$ is trying to collide with. In case $p$ crash, then if the info record in the exchanger implies it is part of an ongoing exchange, then it tries to complete it. Otherwise, either its exchange attempt failed, or that it succeeded and completed by some other process. In the former case, its info record contains an indication for it, as well as the response value.

The use of info records also allows the design of a simple and efficient helping mechanism. In case a collision is done by some process writing its info record second to the exchanger, then any other process can read this record and help to complete the exchange by updating both records with the right response value. Follow that, it can set the exchanger back to EMPTY, that is, to release it. This way, other processes do not need to wait for the two colliding processes to take steps, but can rather complete the exchange on behalf of them, and then use the exchanger again.
